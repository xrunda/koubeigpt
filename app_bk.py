from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma

from langchain.schema import Document
from langchain.document_loaders.csv_loader import CSVLoader
# from langchain.vectorstores import FAISS
# from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
# from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from dotenv import load_dotenv
import streamlit as st
import pandas as pd
import json


from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

from langchain.retrievers.multi_query import MultiQueryRetriever




load_dotenv()
embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# 1.çŸ¢é‡åŒ–æ•°æ®
file_path = 'formatted_documents_split_columns_corrected_100.csv'
data = pd.read_csv(file_path)
docs=[]
for index, row in data.iterrows():
    page_content = row['page_content']
    metadata = row['metadata'].replace("'", '"')
    docs.append(Document(page_content=page_content,metadata=json.loads(metadata)))

# vectorstore = Chroma.from_documents(docs, embeddings)

# loader = CSVLoader("formatted_documents_no_brackets.csv")
# documents = loader.load()
# text_splitter = CharacterTextSplitter(chunk_size=1, chunk_overlap=0)
# docs = text_splitter.split_documents(documents)

# db2 = Chroma.from_documents(docs, embedding_function, persist_directory="./chroma_db_100")


db3 = Chroma(persist_directory="./chroma_db_100", embedding_function=embedding_function)

# loader = CSVLoader(file_path="reshaped_car_data_1000.csv")
# documents = loader.load()
# embeddings = OpenAIEmbeddings()
# db = FAISS.from_documents(documents,embeddings)
# 2.åšç›¸ä¼¼æ€§æœç´¢
def retrieve_info(query):
    # similar_response = db.similarity_search(query,k=3)
    # similar_response = db3.similarity_search(query)

    # retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db3.as_retriever(), llm=llm)
    # unique_docs = retriever_from_llm.get_relevant_documents(query=query)
    # len(unique_docs)

    metadata_field_info = [
        AttributeInfo(
            name="brand",
            description="æ±½è½¦å“ç‰Œ",
            type="string",
        ),
        AttributeInfo(
            name="model",
            description="æ±½è½¦å‹å·",
            type="string",
        ),
        AttributeInfo(
            name="year",
            description="ä¸Šå¸‚å¹´ä»½",
            type="string",
        ),
        AttributeInfo(
            name="price", 
            description="å”®ä»·", 
            type="string"
        ),
        AttributeInfo(
            name="rating", 
            description="è½¦å‹ç‰¹ç‚¹", 
            type="string"
        ),
    ]
    document_content_description = "æ±½è½¦è¯„è®º"
    retriever = SelfQueryRetriever.from_llm(
        llm, db3, document_content_description, metadata_field_info, verbose=True
    )

    print(retriever)
    # page_contents_array = [doc.page_content for doc in retriever]

    return retriever

# custom_prompt = """
#     æˆ‘æƒ³åˆä½œæˆ–å®šåˆ¶æœåŠ¡ï¼Œæ€ä¹ˆè”ç³»ï¼Ÿ
# """
# results=retrieve_info(custom_prompt)
# print(results)

# 3.è®¾ç½®LLMChainå’Œæç¤º

import os
os.environ["DASHSCOPE_API_KEY"] = 'sk-38e455061c004036a70f661a768ba779'
DASHSCOPE_API_KEY='sk-38e455061c004036a70f661a768ba779'
from langchain.llms import Tongyi
from langchain import PromptTemplate, LLMChain

llm = Tongyi(model_kwargs={"api_key":DASHSCOPE_API_KEY},model_name= "qwen-7b-chat-v1")
# llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k-0613')
template = """
    ä½ æ˜¯ä¸€åå¯ä»¥å›ç­”è´­è½¦æ„å‘ç”¨æˆ·ã€å‡†è½¦ä¸»åœ¨ç”¨è½¦ä¹°è½¦æ—¶å€™çš„æ‰€æœ‰é—®é¢˜.
    æˆ‘è®²ç»™ä½ ä¸€ä¸ªç”¨æˆ·å‘æ¥çš„é—®é¢˜ï¼Œå¹¶ä¸”ç»™ä½ ä¸è¿™ä¸ªé—®é¢˜ç›¸å…³çš„å‡ ä¸ªç­”æ¡ˆï¼Œè¿™äº›ç­”æ¡ˆæ¥è‡ªäºçœŸå®çš„è½¦ä¸»åé¦ˆä¿¡æ¯ã€‚
    è¯·ä½ æ ¹æ®è¿™äº›çœŸå®çš„è½¦ä¸»ä¿¡æ¯ï¼Œç»™å‡ºä½ è®¤ä¸ºæœ€ä½³çš„é—®é¢˜ç­”æ¡ˆã€‚
    ä»è€Œå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ï¼Œæé«˜ç”¨æˆ·çš„è´­è½¦æ„å‘ã€‚è§£å†³ç”¨è½¦æ–¹é¢çš„ç–‘é—®ã€‚
    1/ åœ¨ç¯‡å¹…ã€è¯­æ°”ã€é€»è¾‘è®ºè¯å’Œå…¶ä»–ç»†èŠ‚æ–¹é¢ï¼Œç­”å¤åº”ç¬¦åˆæ™ºèƒ½å®¢æœçš„è¦æ±‚å’Œæ ‡å‡†ã€‚
    2/ å¦‚æœä¸æœ€ä½³å®è·µæ— å…³ï¼Œåˆ™åº”å°½é‡æ¨¡ä»¿æœ€ä½³å®è·µçš„é£æ ¼ï¼Œä»¥ä¼ è¾¾æ½œåœ¨å®¢æˆ·çš„ä¿¡æ¯ã€‚
    ä»¥ä¸‹æ˜¯æˆ‘ä»æ½œåœ¨å®¢æˆ·é‚£é‡Œæ”¶åˆ°çš„ä¿¡æ¯ï¼š
    {message}
    ä»¥ä¸‹æ˜¯æ­¤ç±»ç”¨è½¦é—®é¢˜ï¼Œçš„çœŸå®è½¦ä¸»åé¦ˆ
    {best_practice}
    è¯·ç»™å‡ºä½ è®¤ä¸ºæœ€ä½³çš„é—®é¢˜ç­”æ¡ˆï¼š
"""
prompt=PromptTemplate(
    input_variables=["message","best_practice"],
    template=template
)

# llm_chain = LLMChain(prompt=prompt, llm=llm)
chain=LLMChain(llm=llm,prompt=prompt)
# 4.æ£€ç´¢ç”Ÿæˆç»“æœ
def generate_response(message):
    best_practice = retrieve_info(message)
    st.write("message:",message,best_practice)
    response = chain.run(message=message,best_practice=best_practice)
    return response


def main():
    st.set_page_config(
        page_title="ç”¨è½¦å£ç¢‘GPT",page_icon="ğŸš—")

    st.header("ç”¨è½¦å£ç¢‘GPT ğŸš—")
    message = st.text_area("åçˆµMG72010æ¬¾å”®ä»·ï¼Ÿ")
    if message:
        info=st.write("æ­£åœ¨ç”Ÿæˆå›å¤å†…å®¹ï¼Œè¯·ç¨å...")
        result = generate_response(message)
        st.info(result)


if __name__ == "__main__":
    main()

















# 
# save to disk
# db2 = Chroma.from_documents(docs, embedding_function, persist_directory="./chroma_db")
# docs = db2.similarity_search(query)

# # load from disk
# db3 = Chroma(persist_directory="./chroma_db", embedding_function=embedding_function)
# docs = db3.similarity_search(query)
# print(docs[0].page_content)
# https://python.langchain.com/docs/integrations/vectorstores/chroma#basic-example-including-saving-to-disk