import os

from langchain.schema import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

from langchain.chains.question_answering import load_qa_chain

from langchain.vectorstores import Chroma
import pandas as pd
import json
import streamlit as st



os.environ["DASHSCOPE_API_KEY"] = 'sk-38e455061c004036a70f661a768ba779'
DASHSCOPE_API_KEY='sk-38e455061c004036a70f661a768ba779'


embeddings = OpenAIEmbeddings()
vectorstore = Chroma(persist_directory="./chroma_db_modelY", embedding_function=embeddings)

# vectorstore.as_retriever(search_kwargs={'k': 1})
# vectorstore.as_retriever(search_type="mmr")
# vectorstore.as_retriever(search_type="similarity_score_threshold",search_kwargs={'k': 3,'score_threshold': 0.1})



# print(len(vectorstore.get(limit=1)))
from langchain.llms import OpenAI
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

metadata_field_info = [
    AttributeInfo(
        name="brand",
        description="æ±½è½¦å“ç‰Œ",
        type="string",
    ),
    AttributeInfo(
        name="model",
        description="è½¦å‹",
        type="string",
    ),
    AttributeInfo(
        name="name",
        description="å…·ä½“è½¦å‹åç§°",
        type="string",
    ),
    AttributeInfo(
        name="year",
        description="ä¸Šå¸‚å¹´ä»½",
        type="integer",
    ),
    AttributeInfo(
        name="price", 
        description="å”®ä»·", 
        type="string"
    )
]
document_content_description = "æ±½è½¦è½¦å‹çš„ç”¨æˆ·è¯„ä»·"
llm = OpenAI(temperature=0)


retriever = SelfQueryRetriever.from_llm(
    llm, vectorstore, document_content_description, metadata_field_info, verbose=True
)
# ,enable_limit=True

# retriever=SelfQueryRetriever(search_kwargs={"k":3})
# retriever.from_llm(llm=llm,vectorstore=vectorstore,document_content_description=document_content_description,metadata_field_info=metadata_field_info,verbose=True,enable_limit=True)



#âœ”ï¸ å¢åŠ nameå±æ€§
# print(retriever.get_relevant_documents(query="æå…‰L9,2.0Tè‡ªåŠ¨ä¼˜ç‚¹"))

# filter 2.0Tè‡ªåŠ¨ ä¸¢å¤±
# print(retriever.get_relevant_documents(query="æˆ‘æœ€è¿‘è€ƒè™‘ä¹°è½¦ï¼Œç›®å‰å…³æ³¨æå…‰L9è¿™æ¬¾è½¦ï¼Œè¯·ä»‹ç»ä¸€ä¸‹2.0Tè‡ªåŠ¨ä¼˜ç‚¹å’Œç¼ºç‚¹"))

# è¿™ä¸ªå¯ä»¥ï¼Œä¸‰ä¸ªè¿‡æ»¤æ¡ä»¶
# print(retriever.get_relevant_documents(query="è¯·ä»‹ç»æå…‰L9çº¯ç”µåŠ¨,è¿™æ¬¾è½¦çš„ç¼ºç‚¹"))
# å››ä¸ªè¿‡æ»¤æ¡ä»¶å°±ä¸è¡Œäº†ï¼Œç›®å‰æœ€å¤šåªèƒ½ä¸‰ä¸ªè¿‡æ»¤æ¡ä»¶??????ç»“è®ºä¸æ‰å®ï¼Œçº¯ç”µåŠ¨è¿™ä¸ªè¿‡æ»¤æ¡ä»¶ä¸¢å¤±äº†
# print(retriever.get_relevant_documents(query="è¯·ä»‹ç»æå…‰L9çº¯ç”µåŠ¨,è¿™æ¬¾è½¦çš„ç¼ºç‚¹"))


# âœ”ï¸ å¯ä»¥æ‰¾å‡ºç¼ºç‚¹
# print(retriever.get_relevant_documents(query="æå…‰L9çš„ç¼ºç‚¹"))


# âœ”ï¸ å…¨éƒ¨æ‰¾å‡ºæ¥ï¼ŒæŠŠä¼˜ç‚¹æ’å‰é¢ï¼Œç¼ºç‚¹æ’åé¢
# print(retriever.get_relevant_documents(query="ä¸°ç”°å¡ç½—æ‹‰ä¼˜ç‚¹,2020å¹´ä¸Šå¸‚"))

# print(retriever.get_relevant_documents(query="é©¾é©¶è€…ä¹‹è½¦",metadata={"brand": 'ç†æƒ³'}))

# This example only specifies a relevant query
# âœ”ï¸
# print(retriever.get_relevant_documents("å¤§ä¼—é«˜å°”å¤«çš„ä¼˜ç‚¹"))
# âœ”ï¸ 
# print(retriever.get_relevant_documents("2020å¹´ä¹‹åä¸Šå¸‚çš„å®é©¬"))
# print(retriever.get_relevant_documents("2015å¹´ä¹‹åä¸Šå¸‚çš„å®é©¬"))

# 2.æ£€ç´¢ç”Ÿæˆç»“æœ
def retrieve_info(query):
    return retriever.get_relevant_documents(query=query)

# 3.è®¾ç½®LLMChainå’Œæç¤º
llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k-0613')

template = """
    ä½ æ˜¯ä¸€åæŒæ¡äº†å…¨éƒ¨æ±½è½¦ç”¨æˆ·çœŸå®ä½¿ç”¨è¯„ä»·å†…å®¹çš„æ™ºèƒ½å›å¤æœºå™¨äººã€‚
    æˆ‘å°†å‘é€ç»™ä½ ä¸€ä½å®¢æˆ·å…³äºæ±½è½¦ä½¿ç”¨ã€è´­ä¹°å»ºè®®ã€ä¸å…¶ä»–å“ç‰Œè½¦å‹å¯¹æ¯”ç­‰æ–¹é¢çš„é—®é¢˜ã€‚
    å®¢æˆ·å¸Œæœ›ä½ åœ¨çœŸå®è½¦ä¸»è¯„ä»·çš„åŸºç¡€ä¸Šï¼Œå½’çº³æ€»ç»“å½¢æˆä¸€å¥ç»“è®ºæ€§çš„å†…å®¹ï¼Œå‘é€ç»™è¿™ä½å®¢æˆ·ï¼Œå¹¶éµå¾ªä»¥ä¸‹æ‰€æœ‰è§„åˆ™ã€‚
    1/ åœ¨ç¯‡å¹…ã€è¯­æ°”ã€é€»è¾‘è®ºè¯å’Œå…¶ä»–ç»†èŠ‚æ–¹é¢ï¼Œç­”å¤åº”ä¸å°½å¯èƒ½çš„ç»™äººä¸“ä¸šçš„æ„Ÿè§‰ï¼Œå¦‚å®å®¢è§‚çš„è¡¨è¾¾é—®é¢˜çš„ç­”æ¡ˆï¼Œä¸è¦å¢åŠ ä½ è‡ªå·±çš„å¹»è§‰ã€‚
    2/ å¦‚æœåœ¨çœŸå®è½¦ä¸»è¯„ä»·å†…å®¹ä¸­æ²¡æœ‰è¿™ä¸ªé—®é¢˜çš„ç›¸å…³ç­”æ¡ˆï¼Œè¯·å›ç­”ï¼šâ€œå¾ˆæŠ±æ­‰ï¼ŒåŸºäºçœŸå®è½¦ä¸»çš„å£ç¢‘æ•°æ®ï¼Œæˆ‘æš‚æ—¶ä¸èƒ½ç»™å‡ºæ‚¨è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚â€œ
    {message}
    ä»¥ä¸‹æ˜¯é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼ŒçœŸå®è½¦ä¸»è¯„ä»·å†…å®¹ï¼š
    {best_practice}
    è¯·ä¸ºè¿™ä¸ªå®¢æˆ·è¿”å›æœ€ç¬¦åˆé—®é¢˜çš„æœ€ä½³å›å¤å†…å®¹ï¼š

    æ‰€æœ‰å›å¤å‡ä¸ºä¸­æ–‡
"""
prompt=PromptTemplate(
    input_variables=["message","best_practice"],
    template=template
)



st.set_page_config(page_title="æ±½è½¦å£ç¢‘GPT",page_icon="ğŸš—")

chain=LLMChain(llm=llm,prompt=prompt)
# 4.æ£€ç´¢ç”Ÿæˆç»“æœ
def generate_response(message):
    best_practice = retrieve_info(message)

    # st.markdown(f'<small style="color: grey;">å‘é‡å¬å›å†…å®¹ï¼š{best_practice}</small>', unsafe_allow_html=True)
    # è·å–æ¯ä¸ª Document å¯¹è±¡ä¸­çš„ page_content å±æ€§ï¼Œå¹¶å°†å…¶å†…å®¹ç»„åˆä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
    best_practice_text = "<br>".join([doc.page_content for doc in best_practice])

    # åœ¨é¡µé¢ä¸Šä»¥è¾ƒå°çš„å­—ä½“æ‰“å° best_practice_text å˜é‡çš„å†…å®¹ï¼Œå¹¶è®¾ç½®é¢œè‰²ä¸ºæ·¡ç°è‰²
    st.markdown(f'<small style="color: #aaaaaa;">å¬å›å†…å®¹ï¼š<br>{best_practice_text}</small>', unsafe_allow_html=True)




    print('messageï¼š',message)
    print('å‘é‡å¬å›å†…å®¹Lenï¼š',len(best_practice))
    print('å‘é‡å¬å›å†…å®¹ï¼š',best_practice)

    print('')
    print('')
    print('')
    print('')
    print('')
    print('')
    print('')

    # chain_qw = load_qa_chain(llm=llm_qwen, chain_type="stuff",prompt=prompt)
    # chain = load_qa_chain(OpenAI(temperature=0), chain_type="stuff", prompt=prompt)
    # response=chain({"input_documents": best_practice, "question": message}, return_only_outputs=True)



    # response=chain_qw({"input_documents": best_practice, "question": message}, return_only_outputs=True)
    # response=chain.run(input_documents=best_practice, question=message)
    response = chain.run(message=message,best_practice=best_practice)
    return response

# message='ç‰¹æ–¯æ‹‰ModelYçš„åå¤‡ç®±å¯ä»¥æ”¾ä¸‹è‡ªè¡Œè½¦ä¹ˆï¼Ÿ'
# message='ç‰¹æ–¯æ‹‰ModelYçš„åå¤‡ç®±å¯ä»¥æ”¾ä¸‹å†°ç®±ä¹ˆï¼Ÿ'

# å¾ˆæŠ±æ­‰ï¼ŒåŸºäºçœŸå®è½¦ä¸»çš„å£ç¢‘æ•°æ®ï¼Œæˆ‘æš‚æ—¶ä¸èƒ½ç»™å‡ºæ‚¨è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚
# message='ç‰¹æ–¯æ‹‰ModelYå››é©±èƒ½è¶Šé‡ä¹ˆï¼Ÿ'


# message='ç‰¹æ–¯æ‹‰Model Yçš„åå¤‡ç®±å¯ä»¥æ”¾ä¸‹è‡ªè¡Œè½¦ä¹ˆï¼Ÿ'
# print(generate_response(message))

# 5.åˆ›å»ºä¸€ä¸ªåº”ç”¨ä½¿ç”¨streamlitæ¡†æ¶
def main():

    st.header("æ±½è½¦å£ç¢‘GPT ğŸš—")

    message = st.text_area("é—®é—®æˆ‘å§ï¼šæˆ‘çŸ¥é“å…³äºç‰¹æ–¯æ‹‰ModelYçš„ä¸€åˆ‡é—®é¢˜ï¼šå†¬å¤©ç»­èˆªè¡°å‡å¤šå°‘ï¼Ÿåå¤‡ç®±èƒ½æ”¾ä¸‹è‡ªè¡Œè½¦ä¹ˆï¼Ÿ")

    if message:
        
        result_placeholder = st.empty()  # åˆ›å»ºä¸€ä¸ªç©ºä½ï¼Œç”¨äºæ˜¾ç¤ºä¸´æ—¶æ¶ˆæ¯
        result_placeholder.write("æ­£åœ¨ç”Ÿæˆå›å¤å†…å®¹ï¼Œè¯·ç¨å...")

        result = generate_response("ç‰¹æ–¯æ‹‰ModelY"+message)
        
        st.info(result)
        result_placeholder.empty()  # æ¸…ç©ºä¸´æ—¶æ¶ˆæ¯


if __name__ == "__main__":
    main()
